{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLDL_Homework_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_yu8p7jnlhn",
        "colab_type": "text"
      },
      "source": [
        "**Politecnico di Torino**\n",
        "\n",
        "**01TXFSM - Machine learning and Deep learning**\n",
        "\n",
        "**Homework 2**\n",
        "\n",
        "**Alberto Maria Falletta - s277971**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9QcGnGPdX2C",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Install requirements**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9O3aM3Tb28q",
        "colab_type": "code",
        "outputId": "866932c2-4e91-4c8e-c175-53e8d5ce9c6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "!pip3 install 'torch==1.4.0'\n",
        "!pip3 install 'torchvision==0.5.0'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.4.0 in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: torchvision==0.5.0 in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.18.4)\n",
            "Requirement already satisfied: torch==1.4.0 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.4.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (7.0.0)\n",
            "Requirement already satisfied: Pillow-SIMD in /usr/local/lib/python3.6/dist-packages (7.0.0.post3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo942LMOdlh4",
        "colab_type": "text"
      },
      "source": [
        "**Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DokFOdD1dJEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models import alexnet, resnet18, resnet50, resnet34\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cby1Zp0SIhAR",
        "colab_type": "text"
      },
      "source": [
        "**Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kM1_zN1Igpc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_key(in_dict, in_value):\n",
        "  \"\"\"\n",
        "  This function accepts an integer value and returns\n",
        "  the string associated to the class name relating \n",
        "  to the integer, using the class_to_idx dictionary.\n",
        "  \"\"\"\n",
        "  for key, value in in_dict.items(): \n",
        "    if in_value == value: \n",
        "      return key \n",
        "  return \"Key not found!\"\n",
        "\n",
        "\n",
        "def print_occurrences(in_dataset, filename, save=False):\n",
        "  \"\"\"\n",
        "  This function prints horizontal bar-graphs\n",
        "  of occurrences of dataset's images.\n",
        "  Uses get_key to decode the class name\n",
        "  from its integer value.\n",
        "  \"\"\"\n",
        "  in_occurrence_dict = {}\n",
        "  # in_dataset is an object of Caltech class, therefore using .sample \n",
        "  # a list of images and labels is return.\n",
        "  # These lines build a dictionary from in_dataset with labels as key and number\n",
        "  # of occurrences as value\n",
        "  for index in range(0, len(in_dataset)):\n",
        "    img_data, img_label = in_dataset.samples[index]\n",
        "    if img_label not in in_occurrence_dict:\n",
        "      in_occurrence_dict[img_label] = 1\n",
        "    else:\n",
        "      in_occurrence_dict[img_label] += 1\n",
        "\n",
        "  in_y = []\n",
        "  in_x = []\n",
        "\n",
        "  for key in in_occurrence_dict:\n",
        "    in_y.append(get_key(in_dataset.class_to_idx, key)) \n",
        "    in_x.append(in_occurrence_dict[key])\n",
        "\n",
        "  # Plot\n",
        "  fig, ax = plt.subplots(figsize=(14, 25))\n",
        "  ax.barh(in_y, in_x, align='center', alpha=0.5)\n",
        "  ax.set_xlabel('Number of images')\n",
        "  ax.set_ylabel('Classes')\n",
        "  ax.set_title(filename)\n",
        "  for i, v in enumerate(in_x):\n",
        "    plt.text(v+0.2, i, str(v), color='steelblue', va=\"center\")\n",
        "  if save:\n",
        "    plt.savefig(filename + '.png')\n",
        "  plt.show()\n",
        "\n",
        "  return\n",
        "\n",
        "\n",
        "def make_indexes(total_index_list, mode):\n",
        "  \"\"\"\n",
        "  This function splits a list in order to make indexes for training set and\n",
        "  validation set.\n",
        "  The split can be random or based on the original order of the database.\n",
        "  \"\"\"\n",
        "  if mode == 'random':\n",
        "    random.shuffle(total_index_list)\n",
        "    in_train_indexes = total_index_list[:len(total_index_list)//2]\n",
        "    in_val_indexes = total_index_list[len(total_index_list)//2:]\n",
        "\n",
        "  else:\n",
        "    in_train_indexes = []\n",
        "    in_val_indexes = []\n",
        "  \n",
        "    for index in total_index_list:\n",
        "      if index % 2 == 0:\n",
        "        in_train_indexes.append(index)\n",
        "      else:\n",
        "        in_val_indexes.append(index)\n",
        "\n",
        "    # Shuffle the elements\n",
        "    random.shuffle(in_train_indexes)\n",
        "    random.shuffle(in_val_indexes)\n",
        "\n",
        "  return in_train_indexes, in_val_indexes\n",
        "\n",
        "\n",
        "def print_accuracy_loss_plot(in_loss_list, in_accuracy_list, filename):\n",
        "  \"\"\"\n",
        "  This function prints line plots for validation accuracy and loss\n",
        "  for each epoch\n",
        "  \"\"\"\n",
        "  in_epochs = [*range(0, len(in_loss_list))]\n",
        "  in_fig, in_ax = plt.subplots(1, 2, figsize=(14, 7))\n",
        "  in_ax[0].plot(in_epochs, in_loss_list, c='blue', label='Loss')\n",
        "  in_ax[1].plot(in_epochs, in_accuracy_list, c='green', label='Validation Accuracy')\n",
        "  in_ax[0].set_xlabel('Epochs')\n",
        "  in_ax[1].set_xlabel('Epochs')\n",
        "  in_ax[0].set_ylabel('Loss')\n",
        "  in_ax[1].set_ylabel('Validation Accuracy')\n",
        "  plt.savefig(filename + '.png')\n",
        "  plt.show()\n",
        "  return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIDLJuIXK_vh",
        "colab_type": "text"
      },
      "source": [
        "**Set Arguments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5PkYfqfK_SA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
        "\n",
        "NUM_CLASSES = 102  # 101 + 1: There is an extra Background class that should be removed \n",
        "\n",
        "BATCH_SIZE = 128     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
        "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
        "\n",
        "LR = 5e-2            # The initial Learning Rate\n",
        "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default\n",
        "\n",
        "NUM_EPOCHS = 60      # Total number of training epochs (iterations over dataset)\n",
        "STEP_SIZE = 50       # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n",
        "\n",
        "LOG_FREQUENCY = 10\n",
        "\n",
        "pretrained = True  # if training from scratch or finetuning (True, False)\n",
        "network = \"r\"       # if alexnet or resnet (\"a\", \"r\")\n",
        "param = \"fully_connected\"  # parameters to optimize (complete_network\"\", \"fully_connected\", \"convolutional\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gwii0TBHvzh",
        "colab_type": "text"
      },
      "source": [
        "**Define Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUDdw4j2H0Mc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In this code cell is defined a boolean value associated with the \"pretrained\"\n",
        "# hyperparameter of the cell where the network is defined.\n",
        "# In this cell if \"pretrained\" is True Imagenet's mean and std are used in\n",
        "# tranformation phase.\n",
        "\n",
        "if pretrained:\n",
        "  train_transform = transforms.Compose([transforms.Resize(256),      # Resizes short size of the PIL image to 256\n",
        "                                        transforms.CenterCrop(224),  # Crops a central square patch of the image\n",
        "                                                                    # 224 because torchvision's AlexNet needs a 224x224 input!\n",
        "                                                                    # Remember this when applying different transformations, otherwise you get an error\n",
        "                                        # transforms.RandomCrop(224, pad_if_needed=True, padding_mode='edge'),\n",
        "                                        # transforms.RandomHorizontalFlip(p=0.5),\n",
        "                                        # transforms.RandomGrayscale(p=0.1),\n",
        "                                        # transforms.RandomPerspective(distortion_scale=0.5, p=0.5, interpolation=3),\n",
        "                                        # transforms.RandomRotation(30, resample=False, expand=False, center=None, fill=None),\n",
        "                                        # transforms.RandomVerticalFlip(p=0.5),\n",
        "                                        transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
        "  ])\n",
        "\n",
        "  # Define transforms for the evaluation phase\n",
        "  eval_transform = transforms.Compose([transforms.Resize(256),\n",
        "                                        transforms.CenterCrop(224),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])                                  \n",
        "  ])\n",
        "\n",
        "else:\n",
        "  train_transform = transforms.Compose([transforms.Resize(256),      # Resizes short size of the PIL image to 256\n",
        "                                        transforms.CenterCrop(224),  # Crops a central square patch of the image\n",
        "                                                                    # 224 because torchvision's AlexNet needs a 224x224 input!\n",
        "                                                                    # Remember this when applying different transformations, otherwise you get an error\n",
        "                                        transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalizes tensor with mean and standard deviation\n",
        "  ])\n",
        "\n",
        "  # Define transforms for the evaluation phase\n",
        "  eval_transform = transforms.Compose([transforms.Resize(256),\n",
        "                                        transforms.CenterCrop(224),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))                                \n",
        "  ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qYIHPzYLY7i",
        "colab_type": "text"
      },
      "source": [
        "**Prepare Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfVq_uDHLbsR",
        "colab_type": "code",
        "outputId": "51bef887-d0a4-4e02-9b64-13736005e0ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# Clone github repository with data\n",
        "if not os.path.isdir('./Caltech101'):\n",
        "  !git clone https://github.com/albeffe/Homework2-Caltech101.git\n",
        "  !mv 'Homework2-Caltech101' 'Caltech101'\n",
        "\n",
        "DATA_DIR = 'Caltech101/101_ObjectCategories'\n",
        "from my_folder.my_caltech_dataset import Caltech\n",
        "\n",
        "# Prepare Pytorch train/test Datasets\n",
        "train_dataset = Caltech(DATA_DIR, split='train',  transform=train_transform)\n",
        "test_dataset = Caltech(DATA_DIR, split='test', transform=eval_transform)\n",
        "\n",
        "# For visualization purposes\n",
        "visualization_flag = False\n",
        "if visualization_flag:\n",
        "  print('Train Dataset + Validation Dataset: {}'.format(len(train_dataset)), \"\\n\")\n",
        "  print_occurrences(train_dataset, 'Training & Validation Sets', True)\n",
        "  print_occurrences(test_dataset, 'Test Set', True)\n",
        "\n",
        "train_indexes, val_indexes = make_indexes([*range(0, len(train_dataset))], 'equal')\n",
        "val_dataset = Subset(train_dataset, val_indexes)\n",
        "train_dataset = Subset(train_dataset, train_indexes)\n",
        "\n",
        "# Check dataset sizes\n",
        "print('Train Dataset: {}'.format(len(train_dataset)))\n",
        "print('Valid Dataset: {}'.format(len(val_dataset)))\n",
        "print('Test Dataset: {}'.format(len(test_dataset)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Dataset: 2892\n",
            "Valid Dataset: 2892\n",
            "Test Dataset: 2893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYEDQ7Z21ldN",
        "colab_type": "text"
      },
      "source": [
        "**Prepare Dataloaders**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VriRw8SI1nle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbZ1t5Qs2z4j",
        "colab_type": "text"
      },
      "source": [
        "**Prepare Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exHUjtXa22DN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Network selection\n",
        "\n",
        "if network == \"a\":\n",
        "  net = alexnet(pretrained=pretrained)\n",
        "  net.classifier[6] = nn.Linear(4096, NUM_CLASSES) \n",
        "elif network == 'r':\n",
        "  net = resnet18(pretrained=pretrained)\n",
        "  net.fc = nn.Linear(512, NUM_CLASSES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEyL3H_R4qCf",
        "colab_type": "text"
      },
      "source": [
        "**Prepare Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sjq00G94tSc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss() # for classification, we use Cross Entropy\n",
        "\n",
        "# Parameters to optimize:\n",
        "# https://github.com/pytorch/vision/blob/master/torchvision/models/alexnet.py\n",
        "\n",
        "if param == \"complete_network\":\n",
        "  parameters_to_optimize = net.parameters()\n",
        "elif param == \"fully_connected\":\n",
        "  parameters_to_optimize = net.classifier.parameters()\n",
        "elif param == \"convolutional\":\n",
        "  parameters_to_optimize = net.features.parameters()\n",
        "\n",
        "# Optimizers\n",
        "optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "# optimizer = optim.Adam(parameters_to_optimize, lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "# optimizer = optim.Adadelta(parameters_to_optimize, lr=LR, rho=0.9, eps=1e-06, weight_decay=WEIGHT_DECAY)\n",
        "# optimizer = optim.RMSprop(parameters_to_optimize, lr=LR, alpha=0.99, eps=1e-08, weight_decay=WEIGHT_DECAY, momentum=MOMENTUM, centered=False)\n",
        "\n",
        "# Scheduler\n",
        "# A scheduler dynamically changes learning rate\n",
        "# The most common scheduler is the step(-down), which multiplies learning rate by gamma every STEP_SIZE epochs\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxYUli9d9uYQ",
        "colab_type": "text"
      },
      "source": [
        "**Train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcoQ5fD49yT_",
        "colab_type": "code",
        "outputId": "2bf2b808-1ce1-4bea-e508-68ae7aaa7730",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        }
      },
      "source": [
        "net = net.to(DEVICE)\n",
        "\n",
        "cudnn.benchmark # Calling this optimizes runtime\n",
        "\n",
        "current_step = 0\n",
        "loss_list = []\n",
        "valid_accuracy_list = []\n",
        "\n",
        "# Start iterating over the epochs\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))\n",
        "\n",
        "  # Iterate over the dataset\n",
        "  train_running_corrects = 0\n",
        "  for images, labels in train_dataloader:\n",
        "    # Bring data over the device of choice\n",
        "    images = images.to(DEVICE)\n",
        "    labels = labels.to(DEVICE)\n",
        "\n",
        "    net.train() # Sets module in training mode\n",
        "\n",
        "    # PyTorch, by default, accumulates gradients after each backward pass\n",
        "    # We need to manually set the gradients to zero before starting a new iteration\n",
        "    optimizer.zero_grad() # Zero-ing the gradients\n",
        "\n",
        "    # Forward pass to the network\n",
        "    train_outputs = net(images)\n",
        "\n",
        "    # Compute loss based on output and ground truth\n",
        "    loss = criterion(train_outputs, labels)\n",
        "\n",
        "    # Compute gradients for each layer and update weights\n",
        "    loss.backward()  # backward pass: computes gradients\n",
        "    optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "    # Log loss\n",
        "    if current_step % LOG_FREQUENCY == 0:\n",
        "      print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "    current_step += 1\n",
        "\n",
        "  # Step the scheduler\n",
        "  scheduler.step()\n",
        "\n",
        "  loss_list.append(loss.item())\n",
        "\n",
        "  # Validation \n",
        "  net.train(False) # Set Network to evaluation mode\n",
        "  valid_running_corrects = 0\n",
        "  for images, labels in val_dataloader:\n",
        "    images = images.to(DEVICE)\n",
        "    labels = labels.to(DEVICE)\n",
        "\n",
        "    # Forward Pass\n",
        "    valid_outputs = net(images)\n",
        "\n",
        "    # Get predictions\n",
        "    _, valid_preds = torch.max(valid_outputs.data, 1)\n",
        "\n",
        "    # Update Corrects\n",
        "    valid_running_corrects += torch.sum(valid_preds == labels.data).data.item()\n",
        "\n",
        "  # Calculate Accuracy\n",
        "  valid_accuracy = valid_running_corrects / float(len(val_dataset))\n",
        "  print('Validation Accuracy {}'.format(valid_accuracy))\n",
        "  print()\n",
        "  \n",
        "  valid_accuracy_list.append(valid_accuracy)\n",
        "\n",
        "print_accuracy_loss_plot(loss_list, valid_accuracy_list, \"Run_plot\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting epoch 1/60, LR = [0.05]\n",
            "Step 0, Loss 5.090920448303223\n",
            "Step 10, Loss 2.3347580432891846\n",
            "Step 20, Loss 0.9952521920204163\n",
            "Validation Accuracy 0.4529737206085754\n",
            "\n",
            "Starting epoch 2/60, LR = [0.05]\n",
            "Step 30, Loss 0.604526937007904\n",
            "Step 40, Loss 0.4152956008911133\n",
            "Validation Accuracy 0.7627939142461964\n",
            "\n",
            "Starting epoch 3/60, LR = [0.05]\n",
            "Step 50, Loss 0.27601921558380127\n",
            "Step 60, Loss 0.2111152857542038\n",
            "Validation Accuracy 0.809820193637621\n",
            "\n",
            "Starting epoch 4/60, LR = [0.05]\n",
            "Step 70, Loss 0.15590143203735352\n",
            "Step 80, Loss 0.1568639576435089\n",
            "Validation Accuracy 0.8654910096818811\n",
            "\n",
            "Starting epoch 5/60, LR = [0.05]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-d497903af35f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Bring data over the device of choice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxekmR745ySe",
        "colab_type": "text"
      },
      "source": [
        "**Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSHcUqLB5yWO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = net.to(DEVICE)\n",
        "net.train(False)\n",
        "\n",
        "running_corrects = 0\n",
        "for images, labels in tqdm(test_dataloader):\n",
        "  images = images.to(DEVICE)\n",
        "  labels = labels.to(DEVICE)\n",
        "\n",
        "  # Forward Pass\n",
        "  outputs = net(images)\n",
        "\n",
        "  # Get predictions\n",
        "  _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "  # Update Corrects\n",
        "  running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "# Calculate Accuracy\n",
        "accuracy = running_corrects / float(len(test_dataset))\n",
        "\n",
        "print('Test Accuracy: {}'.format(accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}